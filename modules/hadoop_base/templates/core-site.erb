<%#
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-%>
<% @namenode = 
      eval(@nodes).select {|node| node[:roles].include? 'nn'}[0][:hostname] +
      "." + @domain;
   # The list of roles and hive servers that need to proxy for others
   @hive_roles = ['hive-meta', 'hs2', 'hcat'];
   @hive_servers = eval(@nodes).
      select {|node| (node[:roles] & @hive_roles).length > 0}.
      map{|node| node[:hostname] + "." + @domain}.join(",");
  -%>
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration xmlns:xi="http://www.w3.org/2001/XInclude">

<!-- i/o properties -->

  <property>
    <name>io.file.buffer.size</name>
    <value>131072</value>
    <description>The size of buffer for use in sequence files.
  The size of this buffer should probably be a multiple of hardware
  page size (4096 on Intel x86), and it determines how much data is
  buffered during read and write operations.</description>
  </property>

  <property>
    <name>io.serializations</name>
    <value>org.apache.hadoop.io.serializer.WritableSerialization</value>
  </property>

  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.SnappyCodec</value>
    <description>A list of the compression codec classes that can be used
                 for compression/decompression.</description>
  </property>

  <property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
    <description>The implementation for lzo codec.</description>
  </property>

<!-- file system properties -->

  <property>
    <name>fs.default.name</name>
    <value>hdfs://<%= @namenode%>:8020</value>
  </property>

  <property>
    <name>fs.inmemory.size.mb</name>
    <value>100</value>
  </property>

  <property>
    <name>fs.trash.interval</name>
    <value>360</value>
  </property>

  <property>
    <name>fs.checkpoint.dir</name>
    <value><%= @data_dir %>/hdfs/snn</value>
  </property>

  <property>
    <name>fs.checkpoint.edits.dir</name>
    <value>${fs.checkpoint.dir}</value>
  </property>

  <property>
    <name>fs.checkpoint.period</name>
    <value>86400</value>
  </property>

  <property>
    <name>fs.checkpoint.size</name>
    <value>2048000000</value>
  </property>

  <property>
    <name>ipc.client.idlethreshold</name>
    <value>8000</value>
  </property>

  <property>
    <name>ipc.client.connection.maxidletime</name>
    <value>30000</value>
  </property>

  <property>
    <name>ipc.client.connect.max.retries</name>
    <value>50</value>
  </property>

  <!-- Web Interface Configuration -->
  <property>
    <name>webinterface.private.actions</name>
    <value>true</value>
  </property>

  <property>
    <name>hadoop.security.authentication</name>
    <value><%= if @security == "true"
               then "kerberos" else "simple" end %></value>
  </property>

  <property>
    <name>hadoop.security.authorization</name>
    <value>true</value>
  </property>

  <property>
    <name>hadoop.security.use-weak-http-crypto</name>
    <value>false</value>
  </property>

<% if @security == "true" -%>
  <property>
    <name>hadoop.http.authentication.type</name>
    <value>kerberos</value>
  </property>

  <property>
    <name>hadoop.http.filter.initializers</name>
    <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
  </property>

  <property>
    <name>hadoop.http.authentication.cookie.domain</name>
    <value><%= @domain %></value>
  </property>

  <property>
    <name>hadoop.http.authentication.kerberos.principal</name>
    <value>HTTP/<%= @hostname %>.<%= @domain %>@<%= @realm %></value>
  </property>

  <property>
    <name>hadoop.security.auth_to_local</name>
    <value>RULE:[2:$1@$0]([jt]t@<%= @realm %>)s/.*/mapred/
           RULE:[2:$1@$0]([dn]n@<%= @realm %>)s/.*/hdfs/
           DEFAULT</value>
  </property>

  <property>
    <name>hadoop.http.authentication.kerberos.keytab</name>
    <value>/etc/security/hadoop/http.keytab</value>
  </property>

  <property>
    <name>hadoop.http.authentication.signature.secret.file</name>
    <value>/etc/security/hadoop/http-secret</value>
  </property>

  <property>
    <name>hadoop.proxyuser.hive.groups</name>
    <value>users</value>
    <description>Allow the superuser hive to impersonate any members of the 
       group users.</description>
  </property>

  <property>
    <name>hadoop.proxyuser.hive.hosts</name>
    <value><%= @hive_servers %></value>
    <description>Hostname from where superuser hive can connect.</description>
  </property>

<% end -%>
</configuration>
